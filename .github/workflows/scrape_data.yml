# .github/workflows/scrape_data.yml

name: Scrape OSM Data Daily

on:
  # Permite ejecutar este workflow manualmente desde la pestaña Actions en GitHub
  workflow_dispatch:
  # Ejecuta el workflow todos los días a las 5 AM UTC
  schedule:
    - cron: '0 5 * * *'

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest # Usará una máquina virtual de Linux
    steps:
      # 1. Clona tu repositorio para tener acceso al código
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2. Configura el entorno de Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      # 3. Instala las dependencias de Python
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 4. Instala las dependencias del navegador para Playwright
      - name: Install Playwright browsers
        run: playwright install --with-deps chromium

      # 5. Ejecuta el script de scraping
      - name: Run scraping script
        run: python run_update.py
        # Pasa los secretos de GitHub a las variables de entorno del script
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          MI_USUARIO: ${{ secrets.MI_USUARIO }}
          MI_CONTRASENA: ${{ secrets.MI_CONTRASENA }}